{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed002c-398c-4995-8783-49444fd3bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workdir/unsupervised_pretrain/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba638b32-ecd0-414a-b6a9-75a4d54724db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from models import SeriesResNet18\n",
    "from datasets import SeriesEmbedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1de1d7-60c3-4add-bd58-135680abc24b",
   "metadata": {},
   "source": [
    "# Get ready to do some business #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f2ba7-4ec6-410b-a631-37b686946afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SeriesEmbedDataset([\"/datasets/datasets/unsupervised-sentinel2/testset-16SEF/\"], size=512, series_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a882a01-8af4-4a60-9087-e03d1b310f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SeriesEmbedDataset([\"/datasets/datasets/berlin/32UQD/\"], size=512, series_length=8)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f2ef8-729a-4f9a-a1a2-c13cfd2f89ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load the test set, compute embeddings, save embeddings #\n",
    "\n",
    "This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98340e36-ca2a-43b8-8d12-18a9ab932fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b42637-ba94-476a-977c-a899b6570c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/workdir/unsupervised_pretrain/model.pth\", map_location=device).to(device)\n",
    "model = model.eval()\n",
    "autoencoder = torch.load(\"/workdir/unsupervised_pretrain/model-autoencoder.pth\", map_location=device).to(device)\n",
    "autoencoder = autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d5400-2eb1-47d3-9c6b-790e613e5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeddings = []\n",
    "text_embeddings = []\n",
    "skip = 2\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(0, len(ds), skip)):\n",
    "        imagery, _, text_embedding = ds[i]\n",
    "        imagery = torch.unsqueeze(imagery.to(device), dim=0)\n",
    "\n",
    "        visual_embedding = model(imagery)\n",
    "        visual_embedding = F.normalize(visual_embedding, dim=1)\n",
    "\n",
    "        text_embedding = torch.unsqueeze(torch.from_numpy(text_embedding), dim=0).to(device)\n",
    "        text_embedding = F.normalize(text_embedding, dim=1)\n",
    "\n",
    "        visual_embedding = visual_embedding.detach().cpu()\n",
    "        text_embedding = text_embedding.detach().cpu()\n",
    "\n",
    "        visual_embeddings.append(visual_embedding)\n",
    "        text_embeddings.append(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8872a54-aa66-44e8-a109-5e40f09f46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat(text_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d2c98-9078-48d8-9d10-f5d9d7405b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_embeddings, \"/workdir/unsupervised_pretrain/jupyter/text-embeddings.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acc7b1-2fb0-4b35-a380-237e37c4c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeddings = torch.cat(visual_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cec020-6edf-4410-8892-c7fcaf65442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(visual_embeddings, \"/workdir/unsupervised_pretrain/jupyter/visual-embeddings.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc6052-807a-452a-a3b8-6edb31699acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    stuff = autoencoder(F.normalize(visual_embeddings.to(device), dim=1), text_embeddings.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef1f50-3deb-49a8-83c1-da1c67df3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stuff, \"/workdir/unsupervised_pretrain/jupyter/autoencoder-output.t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ead9a8-c851-4eb2-a34f-115dfff812c3",
   "metadata": {},
   "source": [
    "# Load embeddings #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478332d3-286e-43c3-9b85-f24a60d87772",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56eaec3-cdf7-434c-aa9d-5ea1e69688ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/workdir/unsupervised_pretrain/model.pth\", map_location=device).to(device)\n",
    "model = model.eval()\n",
    "autoencoder = torch.load(\"/workdir/unsupervised_pretrain/model-autoencoder.pth\", map_location=device).to(device)\n",
    "autoencoder = autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43e1d7-9ff7-4417-a78d-bb4380518e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_embeddings = torch.load(\"/workdir/unsupervised_pretrain/jupyter/text-embeddings.t\")\n",
    "text_embeddings = _text_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedc82d-5bc7-4d4b-9c9d-c0abf67cbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "_visual_embeddings = torch.load(\"/workdir/unsupervised_pretrain/jupyter/visual-embeddings.t\")\n",
    "visual_embeddings = _visual_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b27229-e2e6-43f4-816e-3c20310afb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_stuff = torch.load(\"/workdir/unsupervised_pretrain/jupyter/autoencoder-output.t\")\n",
    "stuff = [thing.detach().cpu().numpy() for thing in _stuff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11536667-8ce1-4c9a-a6b6-58fff105fbae",
   "metadata": {},
   "source": [
    "## 2D ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bacb01-f4d1-4cc1-95f3-14cba35a4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22425a-5701-4fe5-8a7e-6619c029df0f",
   "metadata": {},
   "source": [
    "### Visual embeddings ###\n",
    "\n",
    "Blue dots are (projections) of original embeddings, orange dots are reconstructed by/from the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b5bc1-ea87-4002-a44d-16045532c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "data0 = tsne.fit_transform(visual_embeddings)\n",
    "data1 = tsne.fit_transform(stuff[0])\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(data0[:, 0], data0[:, 1])\n",
    "plt.scatter(data1[:, 0], data1[:, 1])\n",
    "# plt.scatter(data_2d[[333], 0], data_2d[[333], 1])  # Wood\n",
    "# plt.scatter(data_2d[[82], 0], data_2d[[82], 1])  # City\n",
    "# plt.scatter(data_2d[[440], 0], data_2d[[440], 1])  # Water\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee06662-a612-4b93-a757-ae193d459ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(visual_embeddings, axis=1))), np.max(np.abs(np.mean(stuff[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66dfa3-28b5-47e0-adc2-9803ce3d95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(visual_embeddings - stuff[0], axis=1))), np.mean(np.abs(np.mean(visual_embeddings - stuff[0], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616796e-cf18-46ff-b9ab-2e31c24709a2",
   "metadata": {},
   "source": [
    "### Text embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011441d-1746-4899-871d-91404dcef032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "mask = ~np.isnan(text_embeddings[:, 0])\n",
    "data0 = tsne.fit_transform(text_embeddings[mask])\n",
    "data1 = tsne.fit_transform(stuff[1][mask])\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(data0[:, 0], data0[:, 1])\n",
    "plt.scatter(data1[:, 0], data1[:, 1])\n",
    "# plt.scatter(data_2d[[333], 0], data_2d[[333], 1])  # Wood\n",
    "# plt.scatter(data_2d[[82], 0], data_2d[[82], 1])  # City\n",
    "# plt.scatter(data_2d[[440], 0], data_2d[[440], 1])  # Water\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a0e5a-0d2e-4a9b-ba7e-273d28f1e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(text_embeddings[:, 0])\n",
    "np.max(np.abs(np.mean(text_embeddings[mask], axis=1))), np.max(np.abs(np.mean(stuff[1][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd268b18-bb8d-47d3-a444-c17dc4c9bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(text_embeddings[mask] - stuff[1][mask], axis=1))), np.mean(np.abs(np.mean(text_embeddings[mask] - stuff[1][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6410746-01d4-4a52-b051-a400ade73717",
   "metadata": {},
   "source": [
    "### Shared latent space ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43dcc21-ec33-43f1-bc9f-54e7e9b5ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "mask = ~np.isnan(text_embeddings[:, 0])\n",
    "data0 = tsne.fit_transform(stuff[2][mask])\n",
    "data1 = tsne.fit_transform(stuff[3][mask])\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(data0[:, 0], data0[:, 1])\n",
    "plt.scatter(data1[:, 0], data1[:, 1])\n",
    "# plt.scatter(data_2d[[333], 0], data_2d[[333], 1])  # Wood\n",
    "# plt.scatter(data_2d[[82], 0], data_2d[[82], 1])  # City\n",
    "# plt.scatter(data_2d[[440], 0], data_2d[[440], 1])  # Water\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce308ce-b6d9-4350-a098-d245cf14adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(text_embeddings[:, 0])\n",
    "np.max(np.abs(np.mean(stuff[2][mask], axis=1))), np.max(np.abs(np.mean(stuff[3][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49ded9-2428-403e-97b0-94b2391158ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(np.mean(stuff[2][mask] - stuff[3][mask], axis=1))), np.mean(np.abs(np.mean(stuff[2][mask] - stuff[3][mask], axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9206a18-c8a1-4034-b45a-9a5a2da480a6",
   "metadata": {},
   "source": [
    "# Look for similarity #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e7ff4-b9f9-42ea-b29a-7ac649ed952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc420a-429a-4850-84b5-6eac1cfbf68e",
   "metadata": {},
   "source": [
    "## Utility functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ba2a9-4243-4dcb-b49d-63fe6009d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_query_cosine(query_vector, data, k):\n",
    "    # calculate cosine similarities\n",
    "    cosine_similarities = cosine_similarity(data, query_vector.reshape(1, -1)).flatten()\n",
    "\n",
    "    # get top-k indices\n",
    "    top_k_indices = np.argpartition(-cosine_similarities, k)[:k]\n",
    "    \n",
    "    # return indices of the top-k closest vectors\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb06b9-f85e-4220-b0e6-2e130e965953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_query_l1(query_vector, data, k):\n",
    "    # calculate L1 distances\n",
    "    l1_distances = cdist(data, query_vector.reshape(1, -1), 'cityblock').flatten()\n",
    "\n",
    "    # get top-k indices\n",
    "    top_k_indices = np.argpartition(l1_distances, k)[:k]\n",
    "    \n",
    "    # return indices of the top-k closest vectors\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491b633-c058-419d-9957-0322963e235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_query_l2(query_vector, data, k):\n",
    "    # calculate L2 distances\n",
    "    l1_distances = cdist(data, query_vector.reshape(1, -1), 'euclidean').flatten()\n",
    "\n",
    "    # get top-k indices\n",
    "    top_k_indices = np.argpartition(l1_distances, k)[:k]\n",
    "    \n",
    "    # return indices of the top-k closest vectors\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69188d6-deda-4ec1-999e-03fd4533cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(images, image_number):\n",
    "    # Check that image_number is valid\n",
    "    if image_number < 0 or image_number >= images.shape[0]:\n",
    "        raise ValueError('image_number must be between 0 and the number of images')\n",
    "\n",
    "    # Get the RGB bands (adjusting for 1-based indexing)\n",
    "    r = images[image_number, 3, :, :] # Red band\n",
    "    g = images[image_number, 2, :, :] # Green band\n",
    "    b = images[image_number, 1, :, :] # Blue band\n",
    "\n",
    "    # Stack them along the last dimension to create an RGB image\n",
    "    rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "    # Clamp and scale to [0, 255] range for display\n",
    "    rgb = np.clip(rgb, 0, 2500)  # Ensure values are within 0-2500\n",
    "    rgb = (rgb / 2500) * 255  # Scale values to 0-255\n",
    "\n",
    "    # Convert to 8-bit unsigned integer type\n",
    "    rgb = rgb.astype(np.uint8)\n",
    "\n",
    "    # Show the image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e194a-5e38-4dd0-a660-9842f640bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_images(images):\n",
    "    # Determine the grid size to accommodate all images\n",
    "    grid_size = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "\n",
    "    fig, ax = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "\n",
    "    for i in range(grid_size * grid_size):\n",
    "        if i < images.shape[0]:\n",
    "            # Get the RGB bands (adjusting for 1-based indexing)\n",
    "            r = images[i, 3, :, :]  # Red band\n",
    "            g = images[i, 2, :, :]  # Green band\n",
    "            b = images[i, 1, :, :]  # Blue band\n",
    "\n",
    "            # Stack them along the last dimension to create an RGB image\n",
    "            rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "            # Clamp and scale to [0, 255] range for display\n",
    "            rgb = np.clip(rgb, 0, 2500)  # Ensure values are within 0-2500\n",
    "            rgb = (rgb / 2500) * 255  # Scale values to 0-255\n",
    "\n",
    "            # Convert to 8-bit unsigned integer type\n",
    "            rgb = rgb.astype(np.uint8)\n",
    "\n",
    "            # Display the image\n",
    "            ax[i // grid_size, i % grid_size].imshow(rgb)\n",
    "            ax[i // grid_size, i % grid_size].axis('off')  # Hide the axes\n",
    "        else:\n",
    "            # Hide empty subplots\n",
    "            ax[i // grid_size, i % grid_size].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf7a34-d889-4b64-890f-6f877fb7f3d2",
   "metadata": {},
   "source": [
    "## Visual-visual queries ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4a86e-92b0-4693-8631-f2b724ae8b36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Water ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd2486-6c71-41c1-8a17-217b625a71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = visual_embeddings[440]\n",
    "print(top_k_query_cosine(query_vector, visual_embeddings, 5))\n",
    "print(top_k_query_l1(query_vector, visual_embeddings, 5))\n",
    "print(top_k_query_l2(query_vector, visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfd271-8007-4cd4-927e-0ecb52451c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[440*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b104da-a837-427d-a866-ce4ebeb4b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[394*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df3a22-f23a-44ae-9842-66a017c5bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[393*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e4b97-e31f-4d16-af13-32d2135de361",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[437*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0e136-456c-4fb4-8769-0091866a20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[414*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860d669-5532-4bef-aa4f-467a08c30939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Farmland(?) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6230bb3-3a3d-40f4-b098-2967ade85ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = visual_embeddings[333]\n",
    "print(top_k_query_cosine(query_vector, visual_embeddings, 5))\n",
    "print(top_k_query_l1(query_vector, visual_embeddings, 5))\n",
    "print(top_k_query_l2(query_vector, visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e2e64-9fd7-485e-83fc-d461ac431129",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[333*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717c16-cf26-48c0-866d-7d6dc5df37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[313*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea523640-0427-4cf3-9b59-9d4bffce5580",
   "metadata": {},
   "source": [
    "### Buildings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2faa74b-eda4-453b-b06d-2e7139ad2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = visual_embeddings[19 + 3*21]\n",
    "print(top_k_query_cosine(query_vector, visual_embeddings, 5))\n",
    "print(top_k_query_l1(query_vector, visual_embeddings, 5))\n",
    "print(top_k_query_l2(query_vector, visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37d875-7f5f-44df-8fcb-05c665759a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[(19 + 3*21)*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cedd47-f08e-46db-91c7-e87a6f441ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[104*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99f97e-775e-42e7-abcf-479252b57cbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Text-text queries ##\n",
    "\n",
    "Currently using `instructor-xl` for this.  (The quality of these embeddings can more-or-less be taken as read ðŸ˜‰.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c4764-257d-48e7-abac-d09d76d7cc5b",
   "metadata": {},
   "source": [
    "### Water ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4f30b-add7-41a9-8d43-73b61857216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = text_embeddings[318]\n",
    "print(top_k_query_cosine(query_vector, text_embeddings, 5))\n",
    "print(top_k_query_l1(query_vector, text_embeddings, 5))\n",
    "print(top_k_query_l2(query_vector, text_embeddings, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b883b3-0902-4540-9da4-b9f30ea59628",
   "metadata": {},
   "source": [
    "- Chip 318 is described in the dataset this way: \"Land use land cover: water.\"\n",
    "- Chip 415: ditto\n",
    "- Chip 225: ditto\n",
    "- Chip 142: ditto\n",
    "- Chip 143: ditto\n",
    "- Chip 164: ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae3eee-002e-4823-9c68-a1bce0eded50",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[318*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062046a-00da-48ac-ac0a-13f6a06ab9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[415*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061fa54-028a-494d-993b-82d06d6b07af",
   "metadata": {},
   "source": [
    "### Buildings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eba651-fe29-4719-8f93-97fcb9b6ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = text_embeddings[19 + 3*21]\n",
    "print(top_k_query_cosine(query_vector, text_embeddings, 5))\n",
    "print(top_k_query_l1(query_vector, text_embeddings, 5))\n",
    "print(top_k_query_l2(query_vector, text_embeddings, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385061c-b28c-4d9a-8564-80934c96e031",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Chip 82 (= 19 +3*21) is described in the dataset this way: \"Buildings: numerous. \"\n",
    "- Chip 83: \"Buildings: many. \"\n",
    "- Chip 61: \"Buildings: many. \"\n",
    "- Chip 104: \"Buildings: many. \"\n",
    "- Chip 81: \"Buildings: many. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d68b1-b6f3-4b51-9666-8e4ed2e7ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[61*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030746b4-2188-40f8-819c-481c1e884bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[104*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91073a0e-16db-4443-9baa-a664a007947b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Neighborhood similarity ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063753f3-0d03-4983-bf3e-22bfc1a649f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighborhood_similarity(embeddings1, embeddings2, fn, k):\n",
    "    intersections = []\n",
    "    for embedding1, embedding2 in zip(embeddings1, embeddings2):\n",
    "        neighborhood1 = set(fn(embedding1, embeddings1, k))\n",
    "        neighborhood2 = set(fn(embedding2, embeddings2, k))\n",
    "        intersections.append(len(neighborhood1 & neighborhood2))\n",
    "    return (np.mean(intersections) - 1) / (k - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257df2b-ce1a-4e48-bd8e-2008080e0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_similarity(visual2shared_embeddings, text2shared_embeddings, top_k_query_cosine, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ae7f0-46a4-4c39-99d8-079e59f6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_overlap = []\n",
    "l1_overlap = []\n",
    "for i in tqdm(range(2, 32)):\n",
    "    # Don't Hassle Me I'm Local\n",
    "    cosine_overlap.append(neighborhood_similarity(visual_embeddings, v2t_embeddings, top_k_query_cosine, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d040a2-ccc0-4d56-ba36-050510b5ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(cosine_overlap)\n",
    "plt.plot(l1_overlap)\n",
    "plt.yticks([i * .20 for i in range(0, 5)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9fe66-2e67-4877-aff6-f8a53cad4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_overlap = []\n",
    "l1_overlap = []\n",
    "for i in tqdm(range(2, 32)):\n",
    "    cosine_overlap.append(neighborhood_similarity(ds_embeddings, v2t_embeddings, top_k_query_cosine, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc594f-b2d3-47b3-a026-3e727fc3b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(cosine_overlap)\n",
    "plt.plot(l1_overlap)\n",
    "plt.yticks([i * .20 for i in range(0, 5)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa90ad-d9e0-42d8-bbf9-b2a243e3ed12",
   "metadata": {},
   "source": [
    "## Text queries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af3673-61bc-4287-9c6f-43e424aea3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "embed_model = INSTRUCTOR(\"hkunlp/instructor-xl\").to(device)\n",
    "embed_model.max_seq_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32eb2d3-0c53-48cf-b17b-258034c6922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_visual_query(query_text, instruction, embeddings, k: int = 5):\n",
    "    query = embed_model.encode([[instruction, query_text]])\n",
    "    query = torch.from_numpy(query).to(device)\n",
    "    with torch.inference_mode():\n",
    "        _, z = autoencoder.autoencoder_2(query)\n",
    "        z = z / z.norm(dim=1, keepdim=True)\n",
    "        query = autoencoder.autoencoder_1.decoder(z)\n",
    "        query = query / query.norm(dim=1, keepdim=True)\n",
    "    query.detach().cpu().numpy()\n",
    "    return top_k_query_cosine(query, embeddings, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522232fe-f9ab-4df7-88c8-396f10147a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Represent the geospatial data for retrieval; Input: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e99b4-d542-4a13-b59f-5c2b9270d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_visual_query(\"Buildings: a few. Land use land cover: park.\", instruction, stuff[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63c5df-cc44-4aac-beb4-27b3a7a829f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neighbor = ds[0*2][0]\n",
    "images_neighbor = images_neighbor.detach().cpu().numpy()\n",
    "display_all_images(images_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c32001-6983-4803-9129-0d9b65d1e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(images_neighbor, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b871fc8-3729-4e77-b63e-e1c43a5cb2a4",
   "metadata": {},
   "source": [
    "# Scratch #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d20c3e-9177-460b-981f-0bfbba11bc33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query 1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224ced3-1979-401f-8004-f02229a182cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455e59e-dbae-41bc-a4a9-b4067b77e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embed_model.encode([[instruction, query]])\n",
    "# text_query_embedding /= np.linalg.norm(text_query_embedding, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb4c3e-8374-4ff1-ade7-ecbb2dddd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "\n",
    "for i in range(visual_embeddings.shape[0]):\n",
    "    visual = torch.from_numpy(visual_embeddings[[i]]).to(device)\n",
    "    text = torch.from_numpy(query_embedding).to(device)\n",
    "    with torch.inference_mode():\n",
    "        result = classifier(visual, text).detach().cpu().numpy()\n",
    "    classifications.append(result)\n",
    "\n",
    "classifications = np.concatenate(classifications, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b88f0-20fe-46c3-a8f7-c822681cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c97e40-d1fb-455d-a9ac-c0702e8c21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ca9f7-6ef2-48b5-8c68-389eefcf9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(classifications, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6048a27-58ce-4cd0-8d12-6af083a9b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[377][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b9bef-f743-48ef-ba91-d41f26af71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = visual_embeddings[314]\n",
    "print(top_k_query_cosine(query_vector, visual_embeddings, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a892c9d-d2aa-417b-b667-e2059230a316",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90048d28-86b6-4aeb-9398-18ec7fd52445",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Represent the geospatial data for retrieval; Input: \"\n",
    "query = \"Land use land cover: wood.\"\n",
    "query_embedding = embed_model.encode([[instruction, query]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c26abb-39ca-41e2-bdba-43b86d78253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "\n",
    "for i in range(visual_embeddings.shape[0]):\n",
    "    visual = torch.from_numpy(visual_embeddings[[i]]).to(device)\n",
    "    text = torch.from_numpy(query_embedding).to(device)\n",
    "    with torch.inference_mode():\n",
    "        result = classifier(visual, text).detach().cpu().numpy()\n",
    "    classifications.append(result)\n",
    "\n",
    "classifications = np.concatenate(classifications, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c7ace-06bb-4cd6-914e-0dc918ae51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(classifications, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad436560-4d56-4120-8d92-4ee4f7a851d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_this = ds[183*2][0]\n",
    "images_this = images_this.detach().cpu().numpy()\n",
    "display_all_images(images_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b94917-a9ad-49bb-a4ff-6d1be70e71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf4d56-08d7-460c-baf1-66ccf1ec298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_embeddings.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f2160-229d-4b69-8b02-ffd40816229b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
